{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation (텍스트 번역)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fra-eng/fra.txt', sep = '\\t', names=['eng','fra',' '])\n",
    "df = df.iloc[:70000,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>47940</td>\n",
       "      <td>I like being on my own.</td>\n",
       "      <td>J'apprécie d'être seul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2162</td>\n",
       "      <td>I didn't go.</td>\n",
       "      <td>Je ne m'y suis pas rendu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65199</td>\n",
       "      <td>What do you really think?</td>\n",
       "      <td>Que penses-tu vraiment ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63681</td>\n",
       "      <td>Take as long as you need.</td>\n",
       "      <td>Prends le temps dont tu as besoin !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16267</td>\n",
       "      <td>The doors opened.</td>\n",
       "      <td>Les portes s'ouvrirent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55151</td>\n",
       "      <td>I wanted to read a book.</td>\n",
       "      <td>Je voulais lire un livre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22458</td>\n",
       "      <td>You're very funny.</td>\n",
       "      <td>Vous êtes très marrants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>I don't gamble.</td>\n",
       "      <td>Je ne joue pas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44118</td>\n",
       "      <td>They're in the shower.</td>\n",
       "      <td>Elles sont dans la douche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27435</td>\n",
       "      <td>You were bad at it.</td>\n",
       "      <td>Ça ne vous a pas réussi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35819</td>\n",
       "      <td>I'd like to be alone.</td>\n",
       "      <td>J'aimerais être seule.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29363</td>\n",
       "      <td>I just want answers.</td>\n",
       "      <td>Je veux seulement des réponses.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19486</td>\n",
       "      <td>I was just mugged.</td>\n",
       "      <td>Je viens d'être dévalisé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44676</td>\n",
       "      <td>We have no complaints.</td>\n",
       "      <td>Nous n'enregistrons aucune plainte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20235</td>\n",
       "      <td>Message me online.</td>\n",
       "      <td>Envoie-moi un message en ligne.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10129</td>\n",
       "      <td>Did he go there?</td>\n",
       "      <td>Y est-il allé ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62191</td>\n",
       "      <td>I'd like to learn French.</td>\n",
       "      <td>J'aimerais apprendre le français.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30598</td>\n",
       "      <td>Let the party begin.</td>\n",
       "      <td>Que la fête commence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45747</td>\n",
       "      <td>You're worse than Tom.</td>\n",
       "      <td>Tu es pire que Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59279</td>\n",
       "      <td>Are you planning to stay?</td>\n",
       "      <td>Prévoyez-vous de rester ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             eng                                  fra\n",
       "47940    I like being on my own.              J'apprécie d'être seul.\n",
       "2162                I didn't go.            Je ne m'y suis pas rendu.\n",
       "65199  What do you really think?             Que penses-tu vraiment ?\n",
       "63681  Take as long as you need.  Prends le temps dont tu as besoin !\n",
       "16267          The doors opened.              Les portes s'ouvrirent.\n",
       "55151   I wanted to read a book.            Je voulais lire un livre.\n",
       "22458         You're very funny.             Vous êtes très marrants.\n",
       "7800             I don't gamble.                      Je ne joue pas.\n",
       "44118     They're in the shower.           Elles sont dans la douche.\n",
       "27435        You were bad at it.             Ça ne vous a pas réussi.\n",
       "35819      I'd like to be alone.               J'aimerais être seule.\n",
       "29363       I just want answers.      Je veux seulement des réponses.\n",
       "19486         I was just mugged.            Je viens d'être dévalisé.\n",
       "44676     We have no complaints.  Nous n'enregistrons aucune plainte.\n",
       "20235         Message me online.      Envoie-moi un message en ligne.\n",
       "10129           Did he go there?                      Y est-il allé ?\n",
       "62191  I'd like to learn French.    J'aimerais apprendre le français.\n",
       "30598       Let the party begin.                Que la fête commence.\n",
       "45747     You're worse than Tom.                  Tu es pire que Tom.\n",
       "59279  Are you planning to stay?            Prévoyez-vous de rester ?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fra'] = \"\\t \" + df['fra'].values +\" \\n\"\n",
    "# df['fra']=df['fra'].apply(lambda x:'\\t '+x+' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          \\t Va ! \\n\n",
       "1                                       \\t Salut ! \\n\n",
       "2                                        \\t Salut. \\n\n",
       "3                                       \\t Cours ! \\n\n",
       "4                                      \\t Courez ! \\n\n",
       "                             ...                     \n",
       "69995         \\t Je ne suis jamais monté à cheval. \\n\n",
       "69996    \\t Je ne suis jamais monté sur un cheval. \\n\n",
       "69997               \\t Je n'ai jamais vu Tom ivre. \\n\n",
       "69998          \\t Je n'ai jamais vu d'arc-en-ciel. \\n\n",
       "69999                \\t Je ne t’ai jamais vu rire. \\n\n",
       "Name: fra, Length: 70000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               Go.\n",
       "1                               Hi.\n",
       "2                               Hi.\n",
       "3                              Run!\n",
       "4                              Run!\n",
       "                    ...            \n",
       "69995    I've never ridden a horse.\n",
       "69996    I've never ridden a horse.\n",
       "69997    I've never seen Tom drunk.\n",
       "69998    I've never seen a rainbow.\n",
       "69999    I've never seen you laugh.\n",
       "Name: eng, Length: 70000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['eng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "engVocab = set()\n",
    "fraVocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in df.eng:\n",
    "    for c in line:\n",
    "        engVocab.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'ç',\n",
       " 'é',\n",
       " '’',\n",
       " '€'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in df.fra:\n",
    "    for c in line:\n",
    "        fraVocab.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "engVocabsize = len(engVocab) + 1\n",
    "fraVocabsize = len(fraVocab) + 1\n",
    "print(engVocabsize)\n",
    "print(fraVocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "engVocab = sorted(list(engVocab))\n",
    "fraVocab = sorted(list(fraVocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'ç',\n",
       " 'é',\n",
       " '’',\n",
       " '€']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "engToIndex = dict((c,i+1) for i,c in enumerate(engVocab))\n",
    "fraToIndex = dict((c,i+1) for i,c in enumerate(fraVocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '/': 11,\n",
       " '0': 12,\n",
       " '1': 13,\n",
       " '2': 14,\n",
       " '3': 15,\n",
       " '4': 16,\n",
       " '5': 17,\n",
       " '6': 18,\n",
       " '7': 19,\n",
       " '8': 20,\n",
       " '9': 21,\n",
       " ':': 22,\n",
       " '?': 23,\n",
       " 'A': 24,\n",
       " 'B': 25,\n",
       " 'C': 26,\n",
       " 'D': 27,\n",
       " 'E': 28,\n",
       " 'F': 29,\n",
       " 'G': 30,\n",
       " 'H': 31,\n",
       " 'I': 32,\n",
       " 'J': 33,\n",
       " 'K': 34,\n",
       " 'L': 35,\n",
       " 'M': 36,\n",
       " 'N': 37,\n",
       " 'O': 38,\n",
       " 'P': 39,\n",
       " 'Q': 40,\n",
       " 'R': 41,\n",
       " 'S': 42,\n",
       " 'T': 43,\n",
       " 'U': 44,\n",
       " 'V': 45,\n",
       " 'W': 46,\n",
       " 'X': 47,\n",
       " 'Y': 48,\n",
       " 'Z': 49,\n",
       " 'a': 50,\n",
       " 'b': 51,\n",
       " 'c': 52,\n",
       " 'd': 53,\n",
       " 'e': 54,\n",
       " 'f': 55,\n",
       " 'g': 56,\n",
       " 'h': 57,\n",
       " 'i': 58,\n",
       " 'j': 59,\n",
       " 'k': 60,\n",
       " 'l': 61,\n",
       " 'm': 62,\n",
       " 'n': 63,\n",
       " 'o': 64,\n",
       " 'p': 65,\n",
       " 'q': 66,\n",
       " 'r': 67,\n",
       " 's': 68,\n",
       " 't': 69,\n",
       " 'u': 70,\n",
       " 'v': 71,\n",
       " 'w': 72,\n",
       " 'x': 73,\n",
       " 'y': 74,\n",
       " 'z': 75,\n",
       " 'ç': 76,\n",
       " 'é': 77,\n",
       " '’': 78,\n",
       " '€': 79}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engToIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2], [46, 57, 64, 23], [46, 64, 72, 2], [29, 58, 67, 54, 2], [31, 54, 61, 65, 2], [33, 70, 62, 65, 10]]\n"
     ]
    }
   ],
   "source": [
    "#영어 문장 인코딩\n",
    "encoderInput = []\n",
    "for li in df.eng:\n",
    "    t = []\n",
    "    for c in li :\n",
    "         t.append(engToIndex[c])\n",
    "    encoderInput.append(t)      \n",
    "print(len(encoderInput))\n",
    "print(encoderInput[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2], [1, 3, 43, 73, 61, 3, 26, 3, 2], [1, 3, 83, 53, 3, 53, 64, 67, 70, 71, 105, 4, 3, 2], [1, 3, 27, 73, 3, 58, 57, 73, 3, 4, 3, 2], [1, 3, 82, 3, 64, 9, 53, 61, 56, 57, 105, 4, 3, 2], [1, 3, 45, 53, 73, 72, 57, 14, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "#프랑스 문장 인코딩\n",
    "decoderInput = []\n",
    "for li in df.fra:\n",
    "    t = []\n",
    "    for c in li :\n",
    "         t.append(fraToIndex[c])\n",
    "    decoderInput.append(t)      \n",
    "print(decoderInput[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2], [3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2], [3, 43, 73, 61, 3, 26, 3, 2], [3, 83, 53, 3, 53, 64, 67, 70, 71, 105, 4, 3, 2], [3, 27, 73, 3, 58, 57, 73, 3, 4, 3, 2], [3, 82, 3, 64, 9, 53, 61, 56, 57, 105, 4, 3, 2], [3, 45, 53, 73, 72, 57, 14, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "#프랑스 문장 인코딩\n",
    "decoderFra = []\n",
    "for li in df.fra:\n",
    "    t = []\n",
    "    i = 0\n",
    "    for c in li :\n",
    "        if i > 0:\n",
    "             t.append(fraToIndex[c])\n",
    "        i = i+1\n",
    "    decoderFra.append(t)      \n",
    "print(decoderFra[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장의 길이로 통일 #[3,9,,,15]\n",
    "maxEngLen = max([len(li) for li in df.eng])\n",
    "maxFraLen = max([len(li) for li in df.fra]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderInput = pad_sequences(encoderInput, maxlen = maxEngLen, padding = 'post')\n",
    "decoderInput = pad_sequences(decoderInput, maxlen = maxFraLen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderFra = pad_sequences(decoderFra, maxlen = maxFraLen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30, 64, 10, ...,  0,  0,  0],\n",
       "       [31, 58, 10, ...,  0,  0,  0],\n",
       "       [31, 58, 10, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [32,  7, 71, ..., 63, 60, 10],\n",
       "       [32,  7, 71, ..., 64, 72, 10],\n",
       "       [32,  7, 71, ..., 56, 57, 10]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoderInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 76)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(encoderInput)\n",
    "np.shape(decoderInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3 48 ...  0  0  0]\n",
      " [ 1  3 45 ...  0  0  0]\n",
      " [ 1  3 45 ...  0  0  0]\n",
      " ...\n",
      " [ 1  3 36 ...  0  0  0]\n",
      " [ 1  3 36 ...  0  0  0]\n",
      " [ 1  3 36 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(decoderInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical #인코더 Input, 디코더 Input/ Output\n",
    "\n",
    "encoderInput = to_categorical(encoderInput)\n",
    "decoderInput = to_categorical(decoderInput)\n",
    "decoderFra = to_categorical(decoderFra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 76, 106)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(encoderInput) #(70000줄, 최대 글자수(한 줄당), 글자의 종류)\n",
    "np.shape(decoderInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training시 이전 상태의 실제값을 현재 상태의 decoder 입력으로 해야한다.\n",
    "from keras.layers import Input, Embedding, Dense, LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ex) input = Input(shape=(50,1))\n",
    "feature : 1ro, time-staml(시점): 50개(50갸싹)\n",
    "LSTM(출력)(input)\n",
    "d1 = Dense(10, activation = 'relu')(It)\n",
    "d2 = Dense(1, activation = 'sigmoid')(d1)\n",
    "Model(inputs = ip, outputs=d2)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 56000 samples, validate on 14000 samples\n",
      "Epoch 1/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.7532 - val_loss: 0.6812\n",
      "Epoch 2/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.4688 - val_loss: 0.5466\n",
      "Epoch 3/50\n",
      "56000/56000 [==============================] - 278s 5ms/step - loss: 0.3911 - val_loss: 0.4859\n",
      "Epoch 4/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.3489 - val_loss: 0.4494\n",
      "Epoch 5/50\n",
      "56000/56000 [==============================] - 278s 5ms/step - loss: 0.3214 - val_loss: 0.4253\n",
      "Epoch 6/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.3015 - val_loss: 0.4081\n",
      "Epoch 7/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2861 - val_loss: 0.3963\n",
      "Epoch 8/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2739 - val_loss: 0.3892\n",
      "Epoch 9/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2637 - val_loss: 0.3826\n",
      "Epoch 10/50\n",
      "56000/56000 [==============================] - 278s 5ms/step - loss: 0.2549 - val_loss: 0.3763\n",
      "Epoch 11/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2473 - val_loss: 0.3746\n",
      "Epoch 12/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2404 - val_loss: 0.3715\n",
      "Epoch 13/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2344 - val_loss: 0.3684\n",
      "Epoch 14/50\n",
      "56000/56000 [==============================] - 278s 5ms/step - loss: 0.2288 - val_loss: 0.3659\n",
      "Epoch 15/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2237 - val_loss: 0.3650\n",
      "Epoch 16/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2190 - val_loss: 0.3654\n",
      "Epoch 17/50\n",
      "56000/56000 [==============================] - 278s 5ms/step - loss: 0.2145 - val_loss: 0.3658\n",
      "Epoch 18/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2105 - val_loss: 0.3659\n",
      "Epoch 19/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.2066 - val_loss: 0.3642\n",
      "Epoch 20/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.2030 - val_loss: 0.3665\n",
      "Epoch 21/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1996 - val_loss: 0.3680\n",
      "Epoch 22/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1963 - val_loss: 0.3678\n",
      "Epoch 23/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1932 - val_loss: 0.3691\n",
      "Epoch 24/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.1903 - val_loss: 0.3711\n",
      "Epoch 25/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1875 - val_loss: 0.3729\n",
      "Epoch 26/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.1848 - val_loss: 0.3750\n",
      "Epoch 27/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1823 - val_loss: 0.3756\n",
      "Epoch 28/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1797 - val_loss: 0.3760\n",
      "Epoch 29/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1774 - val_loss: 0.3785\n",
      "Epoch 30/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.1753 - val_loss: 0.3811\n",
      "Epoch 31/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1732 - val_loss: 0.3831\n",
      "Epoch 32/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.1711 - val_loss: 0.3835\n",
      "Epoch 33/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1690 - val_loss: 0.3872\n",
      "Epoch 34/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.1672 - val_loss: 0.3886\n",
      "Epoch 35/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1653 - val_loss: 0.3909\n",
      "Epoch 36/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1635 - val_loss: 0.3921\n",
      "Epoch 37/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1619 - val_loss: 0.3934\n",
      "Epoch 38/50\n",
      "56000/56000 [==============================] - 278s 5ms/step - loss: 0.1602 - val_loss: 0.3969\n",
      "Epoch 39/50\n",
      "56000/56000 [==============================] - 52489s 937ms/step - loss: 0.1586 - val_loss: 0.3991\n",
      "Epoch 40/50\n",
      "56000/56000 [==============================] - 299s 5ms/step - loss: 0.1571 - val_loss: 0.4018\n",
      "Epoch 41/50\n",
      "56000/56000 [==============================] - 292s 5ms/step - loss: 0.1556 - val_loss: 0.4026\n",
      "Epoch 42/50\n",
      "56000/56000 [==============================] - 285s 5ms/step - loss: 0.1540 - val_loss: 0.4062\n",
      "Epoch 43/50\n",
      "56000/56000 [==============================] - 283s 5ms/step - loss: 0.1527 - val_loss: 0.4083\n",
      "Epoch 44/50\n",
      "56000/56000 [==============================] - 282s 5ms/step - loss: 0.1512 - val_loss: 0.4119\n",
      "Epoch 45/50\n",
      "56000/56000 [==============================] - 281s 5ms/step - loss: 0.1500 - val_loss: 0.4121\n",
      "Epoch 46/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1488 - val_loss: 0.4149\n",
      "Epoch 47/50\n",
      "56000/56000 [==============================] - 279s 5ms/step - loss: 0.1475 - val_loss: 0.4166\n",
      "Epoch 48/50\n",
      "56000/56000 [==============================] - 280s 5ms/step - loss: 0.1463 - val_loss: 0.4197\n",
      "Epoch 49/50\n",
      "56000/56000 [==============================] - 282s 5ms/step - loss: 0.1451 - val_loss: 0.4211\n",
      "Epoch 50/50\n",
      "56000/56000 [==============================] - 282s 5ms/step - loss: 0.1440 - val_loss: 0.4217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x18580014d08>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input(shape=(50,1))\n",
    "decoderInputs = Input(shape=(None,fraVocabsize))\n",
    "#디코더입력 = Input(time-step, 프랑스어 문자 종류 수)\n",
    "encoderInputs = Input(shape=(None,engVocabsize))\n",
    "# 인코더입력 = Input( , 영어 문자 종류 수)\n",
    "\n",
    "# 인코더 LSTM 셀|\n",
    "encoderLSTM = LSTM(units=256, return_state=True)\n",
    "# LSTM(출력수=256, return_state=인코더의 마지막 상태 정보를 디코더의 입력 상태 정보로 전달하기 위한 옵션, )\n",
    "\n",
    "# 디코더 LSTM 셀\n",
    "decoderLSTM = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# 인코더 LSTM 셀의 입력 정의\n",
    "_, stateH, stateC = encoderLSTM(encoderInputs)\n",
    "# _, hidden_state, cell 상태\n",
    "# 위로 출력 => stateH, 다른 셀로 전달 stateC\n",
    "\n",
    "encoderStates = [stateH, stateC]\n",
    "# encoderStates : 컨텍스트 벡터\n",
    "\n",
    "decoderOutputs, _, _ = decoderLSTM(decoderInputs, initial_state=encoderStates)\n",
    "# 셀의 관한 상태 정보는 decoder에서는 필요가 없다.\n",
    "decoderSoftmax = Dense(fraVocabsize, activation='softmax')\n",
    "decoderOutputs = decoderSoftmax(decoderOutputs)\n",
    "\n",
    "model = Model(inputs = [encoderInputs,decoderInputs], outputs=decoderOutputs)\n",
    "model.compile(optimizer = 'rmsprop',loss='categorical_crossentropy')\n",
    "model.fit(x = [encoderInput, decoderInput], y = decoderFra, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('translation_eng_to_fra.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nseq2seq동작\\n\\n1) 입력 문장 -> 인코더 -> 은닉상태, 셀상태\\n2) 상태정보와 start 시글널('\\t') 을 디코더로 전달\\n3) 다음 문자를 예측 (stop 시그널 '\\n'이 등장할 때까지 반복)\\n\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "seq2seq동작\n",
    "\n",
    "1) 입력 문장 -> 인코더 -> 은닉상태, 셀상태\n",
    "2) 상태정보와 start 시글널('\\t') 을 디코더로 전달\n",
    "3) 다음 문자를 예측 (stop 시그널 '\\n'이 등장할 때까지 반복)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderModel = Model(inputs=encoderInputs, outputs=encoderStates)\n",
    "\n",
    "#디코더\n",
    "decoderStateInputHidden = Input(shape=(256,))\n",
    "decoderStateInputCell= Input(shape=(256,))\n",
    "decoderStateInputs = [decoderStateInputHidden, decoderStateInputCell]\n",
    "decoderOutputs, stateHidden, stateCell = decoderLSTM(decoderInputs, initial_state = decoderStateInputs)\n",
    "decoderStates = [stateHidden, stateCell]\n",
    "\n",
    "decoderOutputs = decoderSoftmax(decoderOutputs)\n",
    "decoderModel = Model(inputs=[decoderInputs]+decoderStateInputs, outputs=[decoderOutputs]+decoderStates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexToEng = dict((i,c) for c,i in engToIndex.items())\n",
    "indexToFra = dict((i,c) for c,i in fraToIndex.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeSeq(inputSeq): # (1, 26, 80)\n",
    "    \n",
    "    statesValue = encoderModel.predict(inputSeq)\n",
    "    # print(statesValue)\n",
    "    # print(np.shape(statesValue))\n",
    "    \n",
    "    targetSeq = np.zeros((1,1,fraVocabsize)) # 1,1,106\n",
    "    targetSeq[0,0,fraToIndex['\\t']] = 1 # 원핫인코딩\n",
    "    \n",
    "    stop = False\n",
    "    decodedSent=\"\"\n",
    "    while not stop: # \"\\n\"문자를 만날때까지 반복\n",
    "        \n",
    "        output, h, c = decoderModel.predict([targetSeq]+statesValue)\n",
    "        # 예측값을 프랑스 문자로 변환\n",
    "        tokenIndex = np.argmax(output[0,-1,:]) \n",
    "        predChar = indexToFra[tokenIndex]\n",
    "        \n",
    "        # 현시점 예측문자가 예측문장에 추가\n",
    "        decodedSent+=predChar\n",
    "        \n",
    "        if (predChar==\"\\n\" or len(decodedSent)>maxFraLen):\n",
    "            stop = True\n",
    "            \n",
    "        # 현시점 예측결과가 다음 시점에 입력으로 \n",
    "        targetSeq = np.zeros((1,1,fraVocabsize))\n",
    "        targetSeq[0,0,tokenIndex] = 1\n",
    "        \n",
    "        # 현시점 상태를 다음 시점 상태로 사용\n",
    "        statesValue = [h,c]\n",
    "    \n",
    "    return decodedSent # 번역결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장: Hi.\n",
      "정답:  Salut ! \n",
      "번역기:  Salut ! \n",
      "\n",
      "\n",
      "입력문장: I lied.\n",
      "정답:  J'ai menti. \n",
      "번역기:  J'ai manqué. \n",
      "\n",
      "\n",
      "입력문장: Come in.\n",
      "정답:  Entre. \n",
      "번역기:  Entrez ! \n",
      "\n",
      "\n",
      "입력문장: Skip it.\n",
      "정답:  Pas grave. \n",
      "번역기:  Contrôle-lui. \n",
      "\n",
      "\n",
      "입력문장: I did OK.\n",
      "정답:  Je m'en suis bien sortie. \n",
      "번역기:  Je me suis amusé. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seqIndex in [1,50,100,200,300]:\n",
    "    \n",
    "    inputSeq = encoderInput[seqIndex:seqIndex+1]\n",
    "    # print(np.shape(inputSeq)) # (1, 26, 80)\n",
    "    decodedSeq = decodeSeq(inputSeq)\n",
    "    \n",
    "    print(\"입력문장:\", df.eng[seqIndex])\n",
    "    print(\"정답:\", df.fra[seqIndex][1:len(df.fra[seqIndex])-1]) # \"\\t\", \"\\n\" 제거\n",
    "    print(\"번역기:\", decodedSeq[:len(decodedSeq)-1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation ( 텍스트 생성 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"과수원에 있는 배가 맛있다\n",
    "그의 배는 많이 나왔다\n",
    "가는 길에 배를 탔고 오는 길에도 배를 탔다\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'배를': 1,\n",
       " '과수원에': 2,\n",
       " '있는': 3,\n",
       " '배가': 4,\n",
       " '맛있다': 5,\n",
       " '그의': 6,\n",
       " '배는': 7,\n",
       " '많이': 8,\n",
       " '나왔다': 9,\n",
       " '가는': 10,\n",
       " '길에': 11,\n",
       " '탔고': 12,\n",
       " '오는': 13,\n",
       " '길에도': 14,\n",
       " '탔다': 15}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocSize = len(t.word_index)+1 #안덱스 1~15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        seq = encoded[:i+1]\n",
    "        seqs.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3],\n",
       " [2, 3, 4],\n",
       " [2, 3, 4, 5],\n",
       " [6, 7],\n",
       " [6, 7, 8],\n",
       " [6, 7, 8, 9],\n",
       " [10, 11],\n",
       " [10, 11, 1],\n",
       " [10, 11, 1, 12],\n",
       " [10, 11, 1, 12, 13],\n",
       " [10, 11, 1, 12, 13, 14],\n",
       " [10, 11, 1, 12, 13, 14, 1],\n",
       " [10, 11, 1, 12, 13, 14, 1, 15]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(len(i) for i in seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = pad_sequences(seqs, maxlen = maxlen, padding = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  0,  0,  2,  3,  4],\n",
       "       [ 0,  0,  0,  0,  2,  3,  4,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  6,  7],\n",
       "       [ 0,  0,  0,  0,  0,  6,  7,  8],\n",
       "       [ 0,  0,  0,  0,  6,  7,  8,  9],\n",
       "       [ 0,  0,  0,  0,  0,  0, 10, 11],\n",
       "       [ 0,  0,  0,  0,  0, 10, 11,  1],\n",
       "       [ 0,  0,  0,  0, 10, 11,  1, 12],\n",
       "       [ 0,  0,  0, 10, 11,  1, 12, 13],\n",
       "       [ 0,  0, 10, 11,  1, 12, 13, 14],\n",
       "       [ 0, 10, 11,  1, 12, 13, 14,  1],\n",
       "       [10, 11,  1, 12, 13, 14,  1, 15]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = np.array(seqs)\n",
    "x = seqs[:,:-1]\n",
    "y = seqs[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocSize, 10, input_length=maxlen-1)) #입력차원, 출력차원 : 최대길이-1\n",
    "model.add(SimpleRNN(32)) #단어의 임베딩 벡터가 10차원,  32개 히든 상태 \n",
    "model.add(Dense(vocSize, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 2.7568 - accuracy: 0.0769\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 458us/step - loss: 2.7498 - accuracy: 0.0769\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 384us/step - loss: 2.7416 - accuracy: 0.2308\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 461us/step - loss: 2.7328 - accuracy: 0.2308\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 230us/step - loss: 2.7235 - accuracy: 0.2308\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.7138 - accuracy: 0.1538\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.7038 - accuracy: 0.2308\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 2.6933 - accuracy: 0.2308\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.6823 - accuracy: 0.2308\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 2.6710 - accuracy: 0.2308\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 2.6591 - accuracy: 0.3077\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 2.6467 - accuracy: 0.3846\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 229us/step - loss: 2.6338 - accuracy: 0.3846\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.6204 - accuracy: 0.3846\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 228us/step - loss: 2.6065 - accuracy: 0.4615\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 151us/step - loss: 2.5921 - accuracy: 0.4615\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 232us/step - loss: 2.5772 - accuracy: 0.4615\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 155us/step - loss: 2.5618 - accuracy: 0.4615\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.5460 - accuracy: 0.4615\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.5297 - accuracy: 0.4615\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.5130 - accuracy: 0.4615\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 77us/step - loss: 2.4959 - accuracy: 0.4615\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 155us/step - loss: 2.4783 - accuracy: 0.4615\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.4602 - accuracy: 0.4615\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 2.4416 - accuracy: 0.4615\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.4223 - accuracy: 0.3846\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 2.4024 - accuracy: 0.3846\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.3816 - accuracy: 0.3846\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.3600 - accuracy: 0.3846\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.3376 - accuracy: 0.3846\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.3143 - accuracy: 0.3846\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.2902 - accuracy: 0.3846\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.2655 - accuracy: 0.3846\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 2.2402 - accuracy: 0.3846\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.2147 - accuracy: 0.4615\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.1891 - accuracy: 0.4615\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 2.1637 - accuracy: 0.4615\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.1390 - accuracy: 0.4615\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.1147 - accuracy: 0.4615\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.0911 - accuracy: 0.3846\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 2.0681 - accuracy: 0.3846\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 2.0458 - accuracy: 0.3846\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.0237 - accuracy: 0.3846\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 2.0019 - accuracy: 0.3846\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.9801 - accuracy: 0.3846\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 77us/step - loss: 1.9585 - accuracy: 0.3846\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.9370 - accuracy: 0.4615\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 308us/step - loss: 1.9156 - accuracy: 0.4615\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.8944 - accuracy: 0.4615\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.8732 - accuracy: 0.4615\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.8521 - accuracy: 0.4615\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.8312 - accuracy: 0.4615\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 1.8105 - accuracy: 0.4615\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.7901 - accuracy: 0.4615\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.7698 - accuracy: 0.4615\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 385us/step - loss: 1.7495 - accuracy: 0.5385\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.7292 - accuracy: 0.5385\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 230us/step - loss: 1.7089 - accuracy: 0.5385\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.6886 - accuracy: 0.5385\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.6682 - accuracy: 0.5385\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.6477 - accuracy: 0.6154\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.6270 - accuracy: 0.6923\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.6063 - accuracy: 0.6923\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.5855 - accuracy: 0.7692\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.5647 - accuracy: 0.7692\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.5439 - accuracy: 0.7692\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.5230 - accuracy: 0.7692\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.5022 - accuracy: 0.7692\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.4813 - accuracy: 0.7692\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.4606 - accuracy: 0.7692\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.4399 - accuracy: 0.7692\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.4191 - accuracy: 0.7692\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.3984 - accuracy: 0.7692\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.3777 - accuracy: 0.7692\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 155us/step - loss: 1.3571 - accuracy: 0.8462\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.3365 - accuracy: 0.8462\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.3159 - accuracy: 0.8462\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.2954 - accuracy: 0.8462\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.2749 - accuracy: 0.8462\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.2545 - accuracy: 0.8462\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.2341 - accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.2138 - accuracy: 0.8462\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 77us/step - loss: 1.1936 - accuracy: 0.8462\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.1735 - accuracy: 0.9231\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.1534 - accuracy: 0.9231\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 1.1335 - accuracy: 0.9231\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.1138 - accuracy: 0.9231\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.0941 - accuracy: 0.9231\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.0747 - accuracy: 0.9231\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 1.0553 - accuracy: 0.9231\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 1.0362 - accuracy: 0.9231\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.0173 - accuracy: 0.9231\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.9985 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.9799 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.9616 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.9435 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.9256 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.9079 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.8905 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.8733 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.8562 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.8393 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.8228 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.8064 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.7903 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.7745 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.7589 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.7436 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.7285 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.7137 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 0.6992 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.6849 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.6709 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.6572 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.6437 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.6305 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.6176 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.6049 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.5925 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 232us/step - loss: 0.5804 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 308us/step - loss: 0.5685 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.5570 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.5457 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.5346 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.5237 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.5132 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.5029 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.4928 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4830 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.4734 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4641 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4549 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4460 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4373 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4288 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4205 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4125 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.4046 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3969 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3894 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3821 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3750 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 0.3681 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3613 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3547 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3482 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3419 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.3358 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3298 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 0.3239 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3182 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.3127 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 0.3072 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.3019 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2967 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2916 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2867 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.2818 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2771 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.2725 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.2679 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.2635 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2591 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.2549 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.2507 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2467 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2427 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 0.2388 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2350 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2312 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2276 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2240 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2204 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2170 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.2136 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2102 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2070 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2037 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.2006 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1975 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1945 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.1915 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 155us/step - loss: 0.1886 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1857 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.1829 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.1801 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1774 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1747 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1721 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1695 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1669 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 153us/step - loss: 0.1644 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1620 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1595 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1572 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1548 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1525 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1503 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.1481 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 154us/step - loss: 0.1459 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x185bbfc7688>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('배를', 1), ('과수원에', 2), ('있는', 3), ('배가', 4), ('맛있다', 5), ('그의', 6), ('배는', 7), ('많이', 8), ('나왔다', 9), ('가는', 10), ('길에', 11), ('탔고', 12), ('오는', 13), ('길에도', 14), ('탔다', 15)])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentGen(model, t, word, n): #모델, 토크나이져, 입력단어, 예측단어갯수\n",
    "    sent = \"\"\n",
    "    word2 = word\n",
    "    for _ in range(n): #3번 반복\n",
    "        encoded = t.texts_to_sequences([word])[0] #과수원에 \n",
    "        encoded = pad_sequences([encoded], maxlen = 7, padding='pre')\n",
    "        res = model.predict_classes(encoded)\n",
    "        \n",
    "        for w,i in t.word_index.items():\n",
    "            if i == res: #예측 단어와 인덱스가 동일한 단어\n",
    "                break\n",
    "        word = word + \" \" + w #\"과수원에 있는\"(새 단어)\n",
    "        sent = sent+ \" \" + w\n",
    "    print(word2 + sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "과수원에 있는 배가 맛있다\n"
     ]
    }
   ],
   "source": [
    "sentGen(model, t, '과수원에', 3) #과수원에 뒤에 등장하는 3개 단어 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
