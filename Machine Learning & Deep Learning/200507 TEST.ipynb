{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tensorflow와 Keras 라이브러리를 활용한 딥러닝]\n",
    "\n",
    "#### 1. Keras 으로 신경망 설계시 Sequential 및 Dense 클래스 용법에 대해 설명하시요. \n",
    "\n",
    "keras에서 모델을 만드는 방법은 두가지가 있다.\n",
    "Sequential 모델은 순차적으로 레이어 층을 더해주기 때문에 순차 모델이라고 불린다. \n",
    "처음 모델을 생성할시 model = keras.models.Sequential() 객체를 만들어주어 모델을 정의한다. \n",
    "\n",
    "Dense 클래스를 이용하면 n개의 레이어를 쌓아 모델을 만든다.\n",
    "model.add(Dense(1, input_shape=(1,)) 처음 input(입력)으로 들어갈 데이터의 shape or dim(차원)을 정의하고\n",
    "model.add(Dense(16, activation='relu')) 그 밑으로 활성함수와 노드의 갯수를 Dense에 추가하여 쌓아가는 형식의 모델이다.\n",
    "\n",
    "\n",
    "#### Tensorflow 코드를 설명하시요.\n",
    "sess.run(train_step, feed_dict={x:batch_xs, y: batch_ts, keep_prob : 0.5})\n",
    "위에서 만든 session 객체를 실행하는 코드이다.\n",
    "Placeholder연산을 한 train_step을 feed_dict을 통해(graph의 연산에게 x값은 batch_xs로 y값은 batch_ts로) tensor값을 직접 지정해 줍니다. \n",
    "keep_prob은 0.5의 확률만 유지하겠다는 의미이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [강화 학습 기법을 활용한 인공지능 구현]\n",
    "\n",
    "#### 1. 모델피팅(fitting)에 대해 정의하고, 피팅 종류를 나열하시오.\n",
    "모델 핏팅은 모델을 얼마나 학습 데이터에 대해 맞추어 학습시키냐인데, 여기서 생겨날 수 있는 문제둘이 있다.\n",
    "overfitting과 underfitting입니다.\n",
    "Overfitting은 모델이 실제 분포보다 학습 샘플들 분포에 더 근접하게 학습되는 현상을 말한다. 그러다 보면 학습데이터에만 과도하게 학습이 이 이루워져 실제 test데이터에 대해 심한 오차를 보일 수 있다. Overfitting현상을 막기 위해선 정규화 작업 또는 모델이 어느정도 학습되었을때 멈추는 callback(early-stopping)을 주어 Overfitting현상을 방지 할 수 있다.\n",
    "\n",
    "Underfitting은 반대 의미입니다. 모델이 너무 간단하기 때문에 학습 오류가 줄어들지 않는 문제이다. \n",
    "이 두가지의 문제를 해결하기 위해서는 모델 복잡도를 적당하기 주어 Overfitting과 Underfitting 을 방지 해야합니다.\n",
    "\n",
    "#### 2. 정규화 및 표준화에 대해 설명하시오.\n",
    "\n",
    "정규화 : 데이터의 범위를 0과 1사이로 변환하여 데이터의 분포를 조정하는 방법이다. \n",
    "공식 : (해당 값 - 최소값) / (최댓값 - 최소값) \n",
    "ex) MinmaxScaler\n",
    "\n",
    "표준화 : 평균을 기준으로 얼마나 떨어져 있는가를 나타낼 때 사용한다. 값의 스케일이 다른 두개의 변수가 있을때, 이 변수들의 스케일 차이를 제거해 주는 효과가 있다. 평균 0을 기준으로 각 값들의 분산을 나타낸다.\n",
    "공식 :  (해당 값 - 평균) / 표준편차\n",
    "ex) Standard_scaler, Robust_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [CNN과 GAN을 이용한 이미지 데이터 모델링]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosSim(A,B):\n",
    "    return dot(A,B) / (norm(A) * norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "doc1 = np.array([0,1,1,1])\n",
    "doc2 = np.array([1,0,1,1])\n",
    "\n",
    "print(cosSim(doc1, doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. CNN stride를 계산하는 수식 \n",
    "Stride는 입력데이터를 순회하는 필터의 순회간격을 말한다. 만약 (4,4)의 Input을 stride 1씩 순회하며 합성곱하게 되면 총 9번의 합성곱 값이 나와\n",
    "(3*3)의 Feature Map이 생성된다. 즉 stride는 몇칸씩 이동하며 filter와 합성곱을 시킬 것인가 이다.\n",
    "\n",
    "ex) Input =  np.array([[1,0,0,0],[1,1,1,1],[0,1,0,0],[1,1,0,1]]) 와 Filter = np.array([[1,0,0],[0,1,1],[1,1,1]]) 를 합성곱하게 되면\n",
    "Feature Map의 첫번째 요소는 1*1 + 0*0 + 0*0 + 1*0 + 1*1 + 1*1 + 0*1 + 1*1 + 0*1 = 4 가 되는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. pooling layer 동작 과정\n",
    "Pooling layer는 Convolution laye의 출력 데이터를 입력 받아서 출력데이터의 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용된다.\n",
    "Pooling Layer를 처리하는 방법에는 크게 MaxPooling과 AveragePooling으로 나누어진다. \n",
    "MaxPooling은 Feature map을 특정 크기로 잘라낸 후, 그 중 가장 강한 신호(최댓값)을 선택하는 방법이다.\n",
    "AveragePooling은 Feature map을 특정 크기로 잘라낸 후, 평균값을 취하는 방법이다.\n",
    "일반적으로, Pooling 크기와 stride를 같은 크기로 설정하여 모든 원소가 한번 씩 처리되도록 설정합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RNN을 이용한 챗봇 구축]\n",
    "\n",
    "텍스트 전처리는 토큰화 -> 정제 및 정규화 -> 어간 추출-> 불용어제거 -> 정규 표현식 -> 인코딩 -> 단어 분리 과정을 거치게 된다. 각 과정에 대한 간단한 설명 및 예시 코드를 작성하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 토큰의 기준이 무엇이냐에 따라 다르지만 대표적으로 단어 기준으로 토큰화 합니다.\n",
    "NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공합니다. 그 중 word_tokenize와 WordPunctTokenizer를 사용해서 토근화를 해보자면 다음과 같은 결과가 나옵니다.\n",
    "\n",
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "\n",
    "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']  \n",
    "\n",
    "또한 한국어 토큰화를 위한 KONLPY 패키지가 있습니다. 한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히는 형태소단위로 형태소 토큰화를 수행하게 됨을 뜻합니다. 대표적으로 Okt, KOKOMA,등이 있고 더 빠른 분석을 위해선 Mecab도 지원하고 있습니다.\n",
    "\n",
    "from konlpy.tag import Okt  \n",
    "okt=Okt()  \n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "\n",
    "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 정제는 갖고 있는 코퍼스로 부터 노이즈 데이터를 제거하는 것이며, 정규화는 표현 방법이 다른 단어들을 같은 단어로 만들어 줍니다.\n",
    "\n",
    "예를 들면 규칙에 기반한 표기가 다른 단어들의 통합, 대, 소문자 통합,  불필요한 단어의 제거(Removing Unnecessary Words)가 있고, 단어들의 통합으론 대표적으로 Stemming과 Lemmatizaiton이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 어간 추출과 표제어 추출은 정제 작업 중 일종이며 Stemming과 Lemmatization이 있습니다.\n",
    "\n",
    "-Lemmatization : 표제어 추출은 단어들로부터 표제어를 찾아가는 과정입니다. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 형태소는 크게 어간과 접사로 나누어 집니다.\n",
    "NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원합니다.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "n=WordNetLemmatizer()\n",
    "\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print([n.lemmatize(w) for w in words])\n",
    "\n",
    "결과 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
    "\n",
    "-Stemming : 어간을 추출하는 작업을 어간 추출(stemming)이라고 합니다.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "s = PorterStemmer()\n",
    "\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "\n",
    "words=word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "결과 :['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거하는 방법이 있습니다.\n",
    "\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english'))  #nltk에 불용어 사전이 내장되어 있습니다.\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(result) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 정규 표현식 : 특정 규칙이 있는 텍스트 데이터를 빠르게 정제할 수 있습니다.\n",
    "\n",
    "import re  \n",
    "\n",
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\"  \n",
    "\n",
    "re.split('\\s+', text)  #공백기준으로 나누기 \n",
    "\n",
    "re.findall('\\d+',text) #전체 text에서 1개 이상의 숫자 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) 인코딩\n",
    "텍스트보다 수치형을 학습을 잘 할 수 있기 때문에 encoding작업이 필요합니다. 대표적으로 one-hot-encoding, label-encoding이 있습니다.\n",
    "\n",
    "1) onehotencoding\n",
    "\n",
    "pd.get_dummies(train['Sex'])\n",
    "\n",
    "2) labelencoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    " X_train데이터를 이용 피팅하고 라벨숫자로 변환한다\n",
    "X_train['sex'] = encoder.fit_transform(X_train['sex'])\n",
    "\n",
    "encoder.classes_ \n",
    "\n",
    "encoder.inverse_transform(X_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) 단어 분리\n",
    "단어 분리(Subword segmenation) 작업은 하나의 단어는 (단어보다 작은 단위의) 의미있는 여러 내부 단어들(subwords)의 조합으로 구성된 경우가 많기 때문에, 하나의 단어를 여러 내부 단어로 분리해서 단어를 이해해보겠다는 의도를 가진 전처리 작업입니다.\n",
    "\n",
    "- BPE(Byte Pair Encoding)\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
