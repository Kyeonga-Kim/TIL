{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization / LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl=WordNetLemmatizer()\n",
    "wnl.lemmatize(\"watched\", \"v\")\n",
    "wnl.lemmatize(\"dies\", \"v\") #동사만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text=\n",
    "from nltk.stem import PorterStemmer\n",
    "from  nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem(\"electricical\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gon'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem(\"going\")\n",
    "ls.stem(\"gone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['최근', '코로나19로', '인한', '감염으로', '인해', '확진자', '및', '사망자가', '증가하고', '있습니다.코로나', '19를', '이겨냅시다', '.']\n",
      "['코로나19로', '감염으로', '인해', '확진자', '사망자가', '증가하고', '있습니다.코로나', '19를', '이겨냅시다', '.']\n"
     ]
    }
   ],
   "source": [
    "ex=\"\"\"\n",
    "최근 코로나19로 인한 감염으로 인해 확진자 및 사망자가 증가하고 있습니다.코로나 19를 이겨냅시다.\n",
    "\"\"\"\n",
    "stop_words=\"인한 증가 최근 및\"\n",
    "stop_words=stop_words.split(\" \")\n",
    "wt=word_tokenize(ex)\n",
    "print(wt)\n",
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in stop_words:\n",
    "        res.append(w)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is an interpreted, high-level, general-purpose programming language.',\n",
       " \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\",\n",
       " 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "text=sent_tokenize(text)\n",
    "text #문장 단위로 토큰화됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "stopwords.words('english')[:10] #불용어 사전\n",
    "sw=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'interpreted', 'high-level', 'general-purpose', 'programming', 'language'], ['created', 'guido', 'van', 'rossum', 'first', 'released', '1991', 'python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace'], ['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']]\n"
     ]
    }
   ],
   "source": [
    "#모든단어를 소문자,불용어 제거 , 길이가 2이상 \n",
    "voc={}\n",
    "sentences=[]\n",
    "for t in text:\n",
    "    words= word_tokenize(t)\n",
    "    res=[]\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        if word not in sw:\n",
    "            if len(word)>2:\n",
    "                res.append(word)\n",
    "                if word not in voc:\n",
    "                    voc[word]=0\n",
    "                voc[word]+=1\n",
    "    sentences.append(res)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=wi.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocSize=2\n",
    "\n",
    "#단어의 인덱스가 vocSize를 초과하는 단어 추출\n",
    "wordFreq=[w for w,i in wi.items() if i > vocSize]\n",
    "for w in wordFreq:\n",
    "    del wi[w] #3번에 해당된 (vocSize가 2이상)  단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫인코딩\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=okt.morphs(\"나는 자연어 처리를 학습한다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '학습': 5, '한다': 6, '.': 7}\n"
     ]
    }
   ],
   "source": [
    "w2i={}\n",
    "for v in tok:\n",
    "    if v not in w2i.keys():\n",
    "        w2i[v]=len(w2i)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ohe(w, w2i):\n",
    "    ohv=[0]*len(w2i)\n",
    "    index=w2i[w]\n",
    "    ohv[index]=1\n",
    "    return ohv\n",
    "\n",
    "ohe(\"자연어\", w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'데이터': 1, '분석은': 2, '판다스': 3, '최고야': 4, '곰이야': 5}\n"
     ]
    }
   ],
   "source": [
    "#케라스 원핫인코딩  : to_categorical()\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text=\"데이터 분석은 판다스 최고야 곰이야\"\n",
    "\n",
    "tok=Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "print(tok.word_index) #단점: 판다스가, 판다스는 다른 단어로 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=\"판다스 분석은 동물원에서 한다.\"\n",
    "enc=tok.texts_to_sequences([sample])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 분리 (BPE) => 단어 분리에 응용\n",
    "#학습과정에서 사용되지 않은 단어가 테스트 과정\n",
    "\n",
    "\n",
    "#run-length 기법  aaaabbbcc=> a4b3c2\n",
    "#허프만 트리를 이용한 압축\n",
    "\n",
    "#BPE압축 \n",
    "#ex) AABDAABAC\n",
    "#연속적인 글자쌍(2글자)를 구성했을때 가장 많이 등장\n",
    "#AA가 가장 많이 등장  => 다른 소문자 z로 치환 => 또 다음 가장 많이 등장하는 연속 문자 찾기\n",
    "#zABDzABAC\n",
    "#.\n",
    "#.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "dataset=fetch_20newsgroups(shuffle=True,random_state=1, remove=(\"headers\",\"footers\",\"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 토픽과 가장 관령성이 높은 단어를 10개씩 출력\n",
    "import pandas as pd\n",
    "newsDf=pd.DataFrame({\"document\":documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document\n",
       "0      Well i'm not sure about the story nad it did s...\n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
       "2      Although I realize that principle is not one o...\n",
       "3      Notwithstanding all the legitimate fuss about ...\n",
       "4      Well, I will have to change the scoring on my ...\n",
       "...                                                  ...\n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...\n",
       "11310                                                 \\n\n",
       "11311  \\nI agree.  Home runs off Clemens are always m...\n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                        ^^^^^^\\n...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsDf[\"clean_doc\"]=newsDf[\"document\"].str.replace(\"[^a-zA-Z]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3글자 이하 제외\n",
    "newsDf['clean_doc']=newsDf['clean_doc'].apply(lambda x :' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#소문자로 변환\n",
    "newsDf['clean_doc']=newsDf['clean_doc'].apply(lambda x :x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "sw=stopwords.words(\"english\")\n",
    "\n",
    "#토큰화\n",
    "tokenizedDoc=newsDf[\"clean_doc\"].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedDoc=tokenizedDoc.apply(lambda x : [item for item in x if item not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF 메트릭스 구성\n",
    "\n",
    "newsDf[\"clean_doc\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "deTokenizedDoc=[]\n",
    "for i in range(len(newsDf)):\n",
    "    temp=\" \".join(tokenizedDoc[i])\n",
    "    deTokenizedDoc.append(temp)\n",
    "newsDf[\"clean_doc\"]=deTokenizedDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf[\"clean_doc\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vector=TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "res=vector.fit_transform(newsDf[\"clean_doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA (Topic Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdModel=TruncatedSVD(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdModel.fit(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(svdModel.components_) #20개의 토픽과 1000개의 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=vector.get_feature_names() #1000개 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽 1 [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128), ('time', 0.14446), ('thanks', 0.11628), ('make', 0.10882), ('right', 0.10738), ('want', 0.10442)]\n",
      "토픽 2 [('thanks', 0.32877), ('windows', 0.29094), ('card', 0.18069), ('drive', 0.17464), ('mail', 0.15099), ('file', 0.14629), ('advance', 0.12508), ('files', 0.11486), ('software', 0.1135), ('program', 0.10556)]\n",
      "토픽 3 [('game', 0.37186), ('team', 0.32403), ('year', 0.27956), ('games', 0.25339), ('season', 0.18357), ('good', 0.16013), ('players', 0.16), ('play', 0.15089), ('hockey', 0.13785), ('league', 0.12)]\n",
      "토픽 4 [('drive', 0.53398), ('scsi', 0.20212), ('hard', 0.15681), ('disk', 0.156), ('drives', 0.13821), ('card', 0.13546), ('problem', 0.10996), ('controller', 0.10153), ('floppy', 0.09602), ('power', 0.08098)]\n",
      "토픽 5 [('windows', 0.40084), ('file', 0.25485), ('window', 0.16783), ('files', 0.16645), ('program', 0.14452), ('using', 0.12387), ('problem', 0.09694), ('team', 0.09074), ('running', 0.08874), ('version', 0.08428)]\n",
      "토픽 6 [('mail', 0.16703), ('government', 0.16511), ('chip', 0.16457), ('information', 0.13427), ('space', 0.13367), ('encryption', 0.1318), ('data', 0.12497), ('sale', 0.12294), ('clipper', 0.12045), ('phone', 0.11929)]\n",
      "토픽 7 [('like', 0.66864), ('bike', 0.13908), ('know', 0.12271), ('chip', 0.11235), ('sounds', 0.10434), ('looks', 0.07917), ('sure', 0.07246), ('look', 0.06599), ('clipper', 0.0633), ('going', 0.0625)]\n",
      "토픽 8 [('card', 0.44785), ('video', 0.21209), ('sale', 0.21191), ('offer', 0.14728), ('monitor', 0.14596), ('jesus', 0.14174), ('price', 0.14025), ('good', 0.12668), ('condition', 0.11804), ('drivers', 0.11459)]\n",
      "토픽 9 [('know', 0.43223), ('card', 0.34674), ('chip', 0.17827), ('government', 0.15755), ('video', 0.14673), ('people', 0.12839), ('clipper', 0.11011), ('encryption', 0.10446), ('drivers', 0.09901), ('driver', 0.09773)]\n",
      "토픽 10 [('good', 0.46591), ('know', 0.19719), ('time', 0.14637), ('bike', 0.09164), ('jesus', 0.08892), ('think', 0.0762), ('sure', 0.06778), ('used', 0.06733), ('really', 0.06711), ('want', 0.06389)]\n",
      "토픽 11 [('think', 0.8029), ('need', 0.12434), ('mail', 0.12265), ('chip', 0.09648), ('address', 0.09243), ('clipper', 0.0688), ('encryption', 0.06829), ('send', 0.05503), ('data', 0.05455), ('post', 0.05439)]\n",
      "토픽 12 [('know', 0.39594), ('jesus', 0.18837), ('game', 0.14778), ('believe', 0.1302), ('chip', 0.11267), ('bible', 0.11019), ('file', 0.09991), ('christian', 0.0946), ('list', 0.09259), ('sale', 0.08849)]\n",
      "토픽 13 [('people', 0.36626), ('good', 0.30436), ('windows', 0.2602), ('know', 0.20989), ('file', 0.16438), ('sale', 0.1623), ('files', 0.14036), ('price', 0.1171), ('year', 0.11393), ('condition', 0.1069)]\n",
      "토픽 14 [('space', 0.35732), ('know', 0.24778), ('time', 0.21031), ('israel', 0.17871), ('think', 0.17137), ('nasa', 0.13688), ('article', 0.10202), ('card', 0.10014), ('program', 0.09759), ('israeli', 0.09653)]\n",
      "토픽 15 [('good', 0.41583), ('card', 0.25057), ('like', 0.14172), ('space', 0.13414), ('israel', 0.11586), ('year', 0.10349), ('files', 0.09785), ('video', 0.09384), ('file', 0.0898), ('drive', 0.08431)]\n",
      "토픽 16 [('people', 0.5031), ('space', 0.20731), ('problem', 0.19499), ('need', 0.15689), ('good', 0.15567), ('game', 0.12452), ('time', 0.10192), ('using', 0.08475), ('team', 0.08277), ('nasa', 0.0775)]\n",
      "토픽 17 [('time', 0.54924), ('right', 0.20152), ('file', 0.17238), ('bike', 0.15437), ('game', 0.1487), ('card', 0.11914), ('space', 0.10482), ('need', 0.10457), ('mail', 0.09667), ('going', 0.08865)]\n",
      "토픽 18 [('file', 0.39369), ('problem', 0.25833), ('time', 0.2535), ('files', 0.1566), ('used', 0.15048), ('said', 0.13049), ('think', 0.12894), ('israel', 0.10826), ('game', 0.10365), ('armenian', 0.07875)]\n",
      "토픽 19 [('file', 0.36518), ('need', 0.29307), ('right', 0.22763), ('space', 0.21555), ('files', 0.14332), ('power', 0.13595), ('card', 0.11084), ('really', 0.10298), ('game', 0.08866), ('looking', 0.08129)]\n",
      "토픽 20 [('problem', 0.37774), ('mail', 0.37053), ('want', 0.14259), ('windows', 0.14093), ('power', 0.13422), ('address', 0.13202), ('going', 0.12099), ('game', 0.11532), ('send', 0.11457), ('right', 0.11232)]\n"
     ]
    }
   ],
   "source": [
    "def getTopic(c, fName, n=10):\n",
    "    for i , t in enumerate(c):\n",
    "        print(\"토픽 %d\" %(i+1), [(fName[i],t[i].round(5)) for i in t.argsort()[:-n-1:-1]])\n",
    "getTopic(svdModel.components_, terms)                                                                                                                                                                                                                                                                                                                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
